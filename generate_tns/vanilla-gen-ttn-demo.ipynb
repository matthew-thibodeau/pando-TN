{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4566a2f6-f443-4b85-9652-46a65c52aa45",
   "metadata": {},
   "source": [
    "### TTN generation sandbox\n",
    "\n",
    "New method:\n",
    "Instead of using pyGeo, we now just use regular pyTorch.\n",
    "\n",
    "To output the graph, we'll interpret an output vector as probabilities in a graph.\n",
    "Post-processing probably required for:\n",
    "- ensuring graph is connected\n",
    "- ensuring graph is tree-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62c1567b-225e-45e4-b689-64bbe514856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %automagic\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "# import importlib\n",
    "# ttn_package = importlib.import_module(\"../ttn_demo/ttn.py\")\n",
    "\n",
    "# from ..ttn_demo import ttn\n",
    "\n",
    "import ttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b00ee38-cb24-4b9a-b314-b1fdb1e67bfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     45\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(prediction, target)\n\u001b[1;32m     46\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/pando/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pando/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[105], line 23\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Output layer without activation (linear regression)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], output_dim, output_dim)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_dim' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_dim = 2\n",
    "output_dim = 2\n",
    "\n",
    "# Define the model class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through hidden layers with activation\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        # Output layer without activation (linear regression)\n",
    "        return self.fc3(x).view(x.shape[0], output_dim, output_dim)\n",
    "\n",
    "# Sample training data (modify this based on your actual data)\n",
    "input_data = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "output_data = torch.tensor([ [[3.0, 4.0],[3.0, 4.0]],\n",
    "                             [[7.0, 8.0], [7.0, 8.0]],\n",
    "                             [[11.0, 12.0],[11.0, 12.0]]  ]  )\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(input_data, output_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2)  # Adjust batch_size as needed\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = MLP(input_data.shape[1], 16, output_data.shape[1])  # Adjust hidden layer size\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):  # Adjust number of epochs for better training\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Example usage: predict output for a new input\n",
    "new_input = torch.tensor([[7.0, 8.0]])\n",
    "predicted_output = model(new_input)\n",
    "print(f\"Predicted output for new input:\\n {predicted_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "65502976-5111-4655-81ba-1721a0087c64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([-0.30894907,  1.96295376,  0.86974369, -0.27542627,  2.19114059,\n",
      "        1.85649599,  1.64596818,  1.65652284,  2.28712287,  1.94617553,\n",
      "        1.70348118,  2.50535126,  1.62462432,  0.6268707 ,  1.37940695,\n",
      "        0.35255629]), TTN(tensors=31, indices=46)), (array([2.45673348, 0.32151607, 2.20356097, 1.30259196, 1.41356572,\n",
      "       2.94043924, 1.39695988, 1.16426218, 0.83298394, 1.56836296,\n",
      "       1.56442525, 2.92932993, 0.80692259, 0.14826019, 1.57305043,\n",
      "       0.79891753]), TTN(tensors=31, indices=46)), (array([ 0.05627459, -0.5247911 ,  2.23555308,  0.63961334,  2.31403569,\n",
      "        1.21732606,  0.7591391 ,  0.69600629, -1.18423597,  0.54727062,\n",
      "        3.74493231,  1.4457932 ,  1.63832917,  0.47105671,  1.74910897,\n",
      "        1.57566462]), TTN(tensors=31, indices=46)), (array([-0.20873274,  2.11234539,  0.44366383,  2.69534072,  1.02803822,\n",
      "        0.87226619, -1.19261869,  0.19342916,  1.49592225,  2.104682  ,\n",
      "       -0.97143788, -0.32822398,  0.0639892 ,  1.10261315, -0.36425951,\n",
      "        2.28645232]), TTN(tensors=31, indices=46)), (array([-0.16026388,  1.94709727,  1.37481912,  2.12207245, -0.53690536,\n",
      "        1.35259117,  1.6076705 , -0.12183919,  0.86556133,  1.23527432,\n",
      "        1.77735497,  0.79053026,  0.43712243,  1.60803011,  1.0815473 ,\n",
      "        4.38706617]), TTN(tensors=31, indices=46)), (array([-1.06989506,  0.97664431,  1.76992983,  0.39465654,  2.11908708,\n",
      "       -1.24556252, -0.35468793,  2.19976168,  0.35891995,  2.01518516,\n",
      "        2.48790845,  1.06128769,  0.99143857,  1.95859976,  2.41508133,\n",
      "        2.18467589]), TTN(tensors=31, indices=46)), (array([ 3.02372771,  1.28744363,  0.99829811,  1.56575713, -0.8319838 ,\n",
      "        0.41369025,  0.67980689,  1.16232659,  1.26020686,  2.63334028,\n",
      "        0.5683972 ,  0.78775708,  1.3779099 ,  0.2119688 , -0.5506864 ,\n",
      "        1.90153806]), TTN(tensors=31, indices=46)), (array([-1.11510737,  0.47069516,  0.10772209,  1.45880905, -0.05974241,\n",
      "       -1.44560893,  0.43843909,  0.77518319,  1.67934315,  0.27561184,\n",
      "        2.6305883 ,  1.68047969,  0.26520177,  0.34850349,  0.39265497,\n",
      "        1.11556366]), TTN(tensors=31, indices=46)), (array([-0.27491761,  1.00182575,  0.41549195, -0.12425594,  0.86552724,\n",
      "       -0.31987689,  2.38099672, -0.00710045,  1.24314142,  2.92432547,\n",
      "       -0.20302384, -0.19435958,  1.2852265 ,  1.69360911,  1.16135714,\n",
      "       -0.56811755]), TTN(tensors=31, indices=46)), (array([ 1.62442832,  2.04984236, -0.43332098,  1.60919536,  1.72198071,\n",
      "        1.96235527,  2.60383078,  1.06163767, -0.44135949, -0.66838   ,\n",
      "        0.99237408, -0.27782258,  1.59528966, -0.89946645,  1.95245631,\n",
      "        0.37979231]), TTN(tensors=31, indices=46)), (array([ 1.98913872,  0.77821355,  2.66419446,  0.74263846,  2.47090484,\n",
      "        1.00219047,  1.57439084,  1.47257041,  1.38734785, -1.71717774,\n",
      "        1.52596099,  1.47660772,  1.12191513,  0.17772408,  1.43526089,\n",
      "        0.7618759 ]), TTN(tensors=31, indices=46)), (array([ 0.47866368,  1.39559332,  1.12286888,  0.58937012,  1.45830164,\n",
      "        1.6955657 ,  2.06459138,  1.15313941,  2.41077073,  0.05828743,\n",
      "        1.17198541,  0.96585349,  0.66160622,  1.15030079, -0.03904441,\n",
      "        0.73884899]), TTN(tensors=31, indices=46)), (array([ 0.4580525 ,  1.10473254,  0.38387036,  1.32619845,  1.49518898,\n",
      "        0.04357322,  1.06034325,  0.69664368,  2.14432713,  2.07295894,\n",
      "        1.26534007,  1.00325248,  1.40906979,  0.05212629, -1.06690117,\n",
      "        1.15368184]), TTN(tensors=31, indices=46)), (array([ 0.81246762,  0.84613352,  0.06890284,  1.11863377,  1.39849033,\n",
      "        0.5106682 ,  0.7184165 , -0.19127939,  2.09192464,  2.26942349,\n",
      "       -1.31705179,  2.28699444, -1.73006962,  1.06133272,  0.8622025 ,\n",
      "       -0.1165358 ]), TTN(tensors=31, indices=46)), (array([ 2.38271469,  0.42278522,  1.25149838,  0.58196165,  1.33744651,\n",
      "        2.71654245,  1.15552094, -0.1955683 ,  2.24379459,  1.28724761,\n",
      "        1.87780733,  0.50460244,  0.15298879,  0.56485668,  1.23114281,\n",
      "        1.66527988]), TTN(tensors=31, indices=46)), (array([ 1.70981062, -0.90154955, -0.5724099 ,  4.44311362, -0.04426215,\n",
      "        0.44438143,  0.81643235,  0.73786828, -0.04389239,  0.89845948,\n",
      "       -0.16517153,  0.61319063, -0.34636669,  0.69218722,  1.71901376,\n",
      "        0.38661049]), TTN(tensors=31, indices=46)), (array([ 0.8484047 ,  0.9972601 ,  0.90133971,  0.52246311,  1.72334427,\n",
      "        1.27978268,  1.29903874,  0.13364288,  0.18110742, -0.60157903,\n",
      "        1.95557997,  0.75877559,  0.81024071, -0.45555749,  1.18609233,\n",
      "        1.6224535 ]), TTN(tensors=31, indices=46)), (array([ 1.24555062,  1.1699581 ,  1.65475614,  0.23496409,  1.25655911,\n",
      "        0.68693516, -0.4007598 ,  0.04287172,  0.08621136,  0.20031481,\n",
      "        0.11887629,  0.05952646,  2.15131057,  0.70148675,  1.26512374,\n",
      "        1.09504971]), TTN(tensors=31, indices=46)), (array([1.92053026, 2.21623916, 0.15718977, 0.3878647 , 0.97603614,\n",
      "       1.88112446, 3.4397428 , 2.19084031, 1.56457368, 1.03070277,\n",
      "       0.25470758, 0.46402571, 0.71971386, 2.51948012, 2.34444886,\n",
      "       1.11072778]), TTN(tensors=31, indices=46)), (array([-1.1175161 ,  0.70921587,  0.4369431 ,  0.68427252,  0.34118821,\n",
      "        1.06690099,  1.17372917,  0.73227498,  0.68014528,  0.65727856,\n",
      "       -0.45498552,  1.49374285,  1.95950479, -0.4478213 ,  0.67997516,\n",
      "        0.54789723]), TTN(tensors=31, indices=46)), (array([ 1.34732628,  1.5263415 ,  1.17990203,  2.32119496, -0.27508445,\n",
      "        0.11979993,  1.35481172, -0.29703647,  2.04366059,  2.2452794 ,\n",
      "        1.61573013,  2.85705731,  1.68680707,  1.7279111 ,  0.36959011,\n",
      "        0.35160095]), TTN(tensors=31, indices=46)), (array([ 1.58552732,  1.09494994,  1.30915958,  0.24054212,  2.55089062,\n",
      "        0.85823907,  1.53230258,  0.9866457 ,  2.50593521, -1.20325092,\n",
      "        0.15844754,  2.10428948,  0.63594479,  2.1131844 ,  1.5404656 ,\n",
      "        0.53896379]), TTN(tensors=31, indices=46)), (array([ 0.27547277,  1.32388742, -0.10979157,  1.67186941,  0.70608144,\n",
      "        0.59630806,  0.21923342, -0.23231003,  1.99241452,  2.72152809,\n",
      "        1.13240974,  1.34871249,  1.41917273, -0.16389677,  1.43822636,\n",
      "        0.19069399]), TTN(tensors=31, indices=46)), (array([ 0.49578295,  0.38725483,  2.10925896, -0.72754351,  1.65371954,\n",
      "        1.95302671,  1.74647702,  2.02721695,  0.88338008, -0.38426589,\n",
      "        0.83876006,  2.2748163 ,  1.1020721 ,  0.58751158, -0.16992667,\n",
      "        3.52097489]), TTN(tensors=31, indices=46)), (array([ 2.9400559 ,  0.76144759, -0.38073461,  1.68628934,  1.20330316,\n",
      "       -0.68949197,  2.81649258,  0.29518555,  0.5942916 , -1.40001622,\n",
      "        0.07959237,  1.0224399 ,  2.05316827,  1.12107319,  1.0129497 ,\n",
      "        2.33214092]), TTN(tensors=31, indices=46)), (array([ 1.67717878e+00, -7.77698751e-01,  2.86408189e+00, -2.71623767e-01,\n",
      "        1.55901584e+00,  2.66018752e-01, -1.66975738e+00, -3.47428301e-03,\n",
      "        7.69984151e-01,  8.41278500e-05, -9.33998105e-01,  7.82392610e-01,\n",
      "        1.04857191e+00, -2.20188679e-01,  7.24335529e-01,  1.88085075e+00]), TTN(tensors=31, indices=46)), (array([ 0.36527614, -0.17023497,  0.16492922,  1.4312524 , -1.16490437,\n",
      "        0.28494032,  2.17403507,  1.95967532,  0.97213412,  1.96483521,\n",
      "        1.95020684,  0.96930568,  1.26086956,  1.601667  , -0.19143423,\n",
      "        0.76720033]), TTN(tensors=31, indices=46)), (array([1.06912711, 0.25842914, 2.00941299, 0.5275946 , 0.66639916,\n",
      "       1.87931009, 0.60411387, 1.82309437, 0.51303697, 1.32070579,\n",
      "       1.1238281 , 1.95069822, 1.29733276, 0.92782091, 1.59846406,\n",
      "       1.78979043]), TTN(tensors=31, indices=46)), (array([ 1.79660046,  0.56235826,  1.02603397,  0.68302041,  1.145958  ,\n",
      "        1.70580117, -0.24914477,  1.49369792,  0.51470621,  2.66379122,\n",
      "        1.22863324,  1.27074547,  0.32323291,  0.87248037,  3.1101655 ,\n",
      "        1.14527163]), TTN(tensors=31, indices=46)), (array([ 2.90559993e+00,  2.06100516e+00,  7.09730758e-01,  2.77521490e+00,\n",
      "        2.52218417e+00,  8.15369265e-01, -2.14153653e-03,  4.65053223e-01,\n",
      "        1.06504805e+00,  2.40871244e+00,  5.20096203e-01,  1.56550085e+00,\n",
      "        1.83142385e+00,  1.79019867e+00,  1.51573338e+00,  2.12916203e+00]), TTN(tensors=31, indices=46)), (array([ 1.09083762,  0.81036659,  2.35509408,  1.42640791,  2.17390625,\n",
      "        0.00850984,  1.72053966, -0.74693954, -0.40132404,  1.77773067,\n",
      "       -0.34619781,  1.88994853,  0.98501036, -0.22439859,  0.8576071 ,\n",
      "       -0.33266829]), TTN(tensors=31, indices=46)), (array([ 1.59261198,  1.7806863 ,  1.4635172 ,  2.42282551,  3.55309279,\n",
      "        0.97627548,  1.31190391,  2.59354734,  1.70622336,  0.32392344,\n",
      "        1.60178208,  1.30027535, -0.04151298,  1.95599392,  0.14129904,\n",
      "        1.45667449]), TTN(tensors=31, indices=46)), (array([ 0.84148415,  1.53880363,  3.20007334,  0.97183586,  2.38523909,\n",
      "       -0.34627294, -0.32972618,  1.28792482,  1.07060595,  0.79461564,\n",
      "        1.80332829,  1.32597522,  1.30822942,  0.84561761,  1.54868265,\n",
      "        2.89335685]), TTN(tensors=31, indices=46)), (array([ 0.94758449,  2.63119853,  0.49286214,  1.17992071,  0.13126679,\n",
      "        1.21165521, -0.99677114,  1.2221148 ,  1.08546416,  2.0589748 ,\n",
      "        2.38905766,  0.4405258 ,  0.07986562,  2.01915801,  1.05475268,\n",
      "        1.23935085]), TTN(tensors=31, indices=46)), (array([ 2.34305186,  1.64646967, -0.17703191,  0.39545868,  1.57875712,\n",
      "        0.02510588,  1.861591  ,  0.89900192,  1.12001212,  2.21816545,\n",
      "        2.03191766,  0.03681126,  1.50599426,  0.8835346 ,  1.56978567,\n",
      "        0.75743772]), TTN(tensors=31, indices=46)), (array([ 0.40559366,  1.48044567,  1.21041522, -0.15037119,  0.11699203,\n",
      "        0.10905486,  1.78453601, -0.03089042,  1.01562839,  0.15313606,\n",
      "        2.17291336,  0.35926939,  0.47891144,  1.16587128,  0.61473644,\n",
      "        1.69670442]), TTN(tensors=31, indices=46)), (array([ 1.93219587,  0.02765399,  0.97079025,  0.50792928, -0.71324776,\n",
      "        1.57234165,  0.81466986,  1.21000071,  1.70690933,  0.79231571,\n",
      "        0.3328219 ,  0.91052363,  0.18910185,  2.01068249,  2.46896294,\n",
      "        2.7117996 ]), TTN(tensors=31, indices=46)), (array([ 1.42992622,  1.15885936,  1.570098  ,  2.31314012,  1.30254655,\n",
      "        2.52206962,  0.03321532,  0.77644764,  1.69528902,  2.58778577,\n",
      "       -0.71593672,  0.40098111,  1.43765092,  1.17243969,  0.13607015,\n",
      "        0.47564661]), TTN(tensors=31, indices=46)), (array([ 1.3791285 ,  2.38494756,  2.93080663,  0.09067575, -0.5064775 ,\n",
      "        1.32540608,  0.11378512, -0.00956774,  0.92728284, -0.56394556,\n",
      "        1.80662266,  1.87157762,  0.92664671,  0.75371739,  0.7074384 ,\n",
      "        0.92863869]), TTN(tensors=31, indices=46)), (array([ 0.02685563,  2.02325365,  0.56147826,  2.23864247,  0.87218524,\n",
      "       -0.05849588,  1.01720984,  2.1678583 ,  0.52174171,  0.49799395,\n",
      "        1.02590132,  0.82525417,  1.10284038,  1.29505868,  0.12619715,\n",
      "        0.49575099]), TTN(tensors=31, indices=46)), (array([ 0.38288059,  1.83979801,  0.62275806, -0.32653597,  0.3730065 ,\n",
      "        2.03068892,  0.08523624,  2.95381725,  0.60731486,  0.97176426,\n",
      "        0.68193334,  0.54904973,  2.58871364,  1.96368937,  0.80696925,\n",
      "        3.59997103]), TTN(tensors=31, indices=46)), (array([ 2.38648099,  0.01749701, -0.78365612, -0.11766044,  1.71193218,\n",
      "        0.0297638 ,  2.51911516, -0.28191192,  0.53326368,  2.85476309,\n",
      "        2.14039931, -0.71626667,  0.4750356 , -0.21861831,  1.27265148,\n",
      "        0.595727  ]), TTN(tensors=31, indices=46)), (array([ 1.47194103,  0.91578314,  1.57557166,  1.38050161, -0.68548904,\n",
      "        0.00656553,  1.69537807,  1.04225769, -0.71983597,  0.12639   ,\n",
      "        1.7385091 ,  0.9873285 ,  1.47819851, -1.42491236,  2.40365184,\n",
      "        0.46491839]), TTN(tensors=31, indices=46)), (array([ 2.03172710e+00,  1.79139978e+00,  1.94968722e-03,  1.79597388e+00,\n",
      "        3.10842623e+00,  1.93118679e-01,  9.82996145e-01,  1.33646635e+00,\n",
      "       -1.85428147e-01,  9.94422041e-01,  7.86054499e-02,  6.51898860e-01,\n",
      "        2.00599542e+00,  2.35164728e+00, -1.84791073e+00, -3.17979094e-01]), TTN(tensors=31, indices=46)), (array([ 1.47587889,  1.89857615,  2.18252809,  1.79254727,  0.19394352,\n",
      "       -0.20857114,  0.73448324,  0.53144498,  2.31097185,  0.94551243,\n",
      "        0.42706412,  1.61463584,  1.43723112,  0.05415968,  0.10110632,\n",
      "        0.68329949]), TTN(tensors=31, indices=46)), (array([ 0.27582781,  1.20591901, -1.00660433,  2.17100023,  0.93683487,\n",
      "        2.0729065 ,  1.55042867,  2.3549158 , -0.85333955,  0.36903586,\n",
      "        2.83836686,  2.59302666, -0.35578699,  0.5883158 ,  2.60134347,\n",
      "        1.99268153]), TTN(tensors=31, indices=46)), (array([ 0.38051519,  0.71410562,  1.4132711 ,  1.70795204,  3.40585577,\n",
      "        1.08298612,  0.21969066,  2.58242206,  0.24772915, -0.2792153 ,\n",
      "        1.84131445, -0.1442833 ,  1.05369229,  0.35782769,  0.70445232,\n",
      "       -0.80459233]), TTN(tensors=31, indices=46)), (array([-0.07908629,  0.72913055,  2.25003388,  1.0007081 ,  1.48740861,\n",
      "        1.42727449,  0.35482992,  2.80593357,  0.4102468 ,  1.76338408,\n",
      "       -0.24971759,  1.21839218, -0.37443461,  1.74861643,  0.83892357,\n",
      "        2.69705308]), TTN(tensors=31, indices=46)), (array([ 0.53707083,  0.16946523,  0.23064079,  0.70138571,  1.8140435 ,\n",
      "        3.18984988,  2.29268496,  1.97504186,  0.59590851,  0.38696526,\n",
      "       -0.23330416,  0.16850019,  2.1931337 , -0.59409032,  1.36709764,\n",
      "        0.46815603]), TTN(tensors=31, indices=46)), (array([ 2.13561769,  0.35037738,  2.5162244 ,  0.05731211,  1.18829923,\n",
      "        3.03789198,  0.99602087,  1.09141084, -0.62208683,  0.99753759,\n",
      "       -0.10686177, -0.04297489, -0.29010337,  1.51962828, -0.65717204,\n",
      "       -0.09411222]), TTN(tensors=31, indices=46)), (array([ 3.1621565 ,  0.66885157, -0.25747095,  0.06170359,  1.89777879,\n",
      "        0.98085234,  0.97250146,  2.45573348, -0.72806804,  2.82800106,\n",
      "        2.72965068,  0.58278221,  1.64697645,  0.22107024, -0.59222838,\n",
      "        0.89067389]), TTN(tensors=31, indices=46)), (array([0.72727748, 0.86822817, 1.55346988, 1.89763694, 0.25839287,\n",
      "       1.02469663, 1.81110511, 0.70102072, 2.3455758 , 2.61655371,\n",
      "       0.19482252, 0.92304538, 2.58758158, 1.6153474 , 0.91798657,\n",
      "       2.58895693]), TTN(tensors=31, indices=46)), (array([ 4.04997551,  0.58702429,  1.66578108,  1.93412612,  1.74171288,\n",
      "       -0.1424559 , -0.02241364,  1.05029162,  0.41411357,  0.60698639,\n",
      "        1.43114715,  2.03088304, -0.61772967,  2.66026758,  3.00159858,\n",
      "        1.1176474 ]), TTN(tensors=31, indices=46)), (array([ 0.53635541,  3.92102061,  0.08138017,  2.31649169,  3.00101684,\n",
      "        0.37216164,  1.46546757,  0.76447473,  2.26967845,  0.96846232,\n",
      "        0.85707689,  1.70114286,  0.5199069 , -0.24523887, -0.269049  ,\n",
      "        1.67460776]), TTN(tensors=31, indices=46)), (array([-1.12129912,  1.13678771,  1.94064444, -0.19870151,  1.49110209,\n",
      "        2.41200859,  1.47071642,  0.79020646, -1.2272191 ,  2.30534364,\n",
      "       -0.06289274,  0.51642569,  0.68709916,  0.58939947, -0.55945376,\n",
      "        1.73309981]), TTN(tensors=31, indices=46)), (array([ 0.75106391,  0.47168419, -0.59939354, -0.01681062,  1.49252996,\n",
      "        0.38901209,  0.29002347,  2.33943009,  1.88612778,  0.1170707 ,\n",
      "        2.18672724,  0.1382678 ,  0.9384737 ,  1.28614558,  3.23666341,\n",
      "        0.44919691]), TTN(tensors=31, indices=46))]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Matt`s TTN data is in \"/Users/nicolassawaya/proj/pando/pando-TN-repo/ttn_learning/data/learning_pairs\"\n",
    "'''\n",
    "# path_ttn_data = \"/Users/nicolassawaya/proj/pando/pando-TN-repo/ttn_learning/data/learning_pairs/\"\n",
    "path_ttn_data = \"../ttn_learning/data/learning_pairs/\"\n",
    "fname_ttn_L16 = path_ttn_data+\"pairs_L16_D2_m3.pkl\"\n",
    "fname_ttn_L24 = path_ttn_data+\"pairs_L24_D2_m3.pkl\"\n",
    "\n",
    "with open(fname_ttn_L16, \"rb\") as f:\n",
    "    data_L16 = pickle.load(f)\n",
    "with open(fname_ttn_L24, \"rb\") as f:\n",
    "    data_L24 = pickle.load(f)\n",
    "    \n",
    "print(data_L16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "362c214a-2c04-422e-b90a-f065730a9e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "3\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'ttn.TTN'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 3], dtype=int16),\n",
       " array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
       "        3, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 3, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        3, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 3], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 3, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0,\n",
       "        0, 3, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "        0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 3], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0,\n",
       "        0, 0, 0], dtype=int16),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3,\n",
       "        0, 0, 0], dtype=int16)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjmat = data_L24[8][1].get_adjacency_matrix()\n",
    "\n",
    "# plt.spy( adjmat )\n",
    "print(adjmat[:,2])\n",
    "print( adjmat.max() )\n",
    "\n",
    "print(type(data_L24[0][0]))\n",
    "print(type(data_L24[0][1]))\n",
    "\n",
    "list( data_L24[0][0] )\n",
    "list( data_L24[0][1].get_adjacency_matrix() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "257ccea7-dd38-473f-b736-017c9e5478fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthew_to_torch(learning_pairs):\n",
    "    \"\"\"Takes Matt's 'learning pairs' data into pytorch format\"\"\"\n",
    "    \n",
    "    # input_data = torch.tensor( [[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    # output_data = torch.tensor([ [[3.0, 4.0], [3.0, 4.0]],\n",
    "    #                              [[7.0, 8.0], [7.0, 8.0]],\n",
    "    #                              [[11.0, 12.0], [11.0, 12.0]]  ]  )\n",
    "    \n",
    "    list_couplings = []\n",
    "    list_adjmats = []\n",
    "\n",
    "    # inp_tensor = torch.tensor()\n",
    "    # out_tensor = torch.tensor()\n",
    "\n",
    "    ctr=0\n",
    "    for pair in learning_pairs:\n",
    "        list_couplings.append( torch.tensor(pair[0]) )\n",
    "        list_adjmats.append( torch.tensor(pair[1].get_adjacency_matrix()) )\n",
    "        # ctr += 1\n",
    "        # if ctr>2:\n",
    "        #     break\n",
    "    \n",
    "    return torch.stack(list_couplings), torch.stack(list_adjmats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "097fe919-d9ea-4289-9c35-9be5cfb3c93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 16])\n",
      "torch.Size([56, 31, 31])\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float32\n",
      "torch.float32\n",
      "tensor([[-3.0895e-01,  1.9630e+00,  8.6974e-01, -2.7543e-01,  2.1911e+00,\n",
      "          1.8565e+00,  1.6460e+00,  1.6565e+00,  2.2871e+00,  1.9462e+00,\n",
      "          1.7035e+00,  2.5054e+00,  1.6246e+00,  6.2687e-01,  1.3794e+00,\n",
      "          3.5256e-01],\n",
      "        [ 2.4567e+00,  3.2152e-01,  2.2036e+00,  1.3026e+00,  1.4136e+00,\n",
      "          2.9404e+00,  1.3970e+00,  1.1643e+00,  8.3298e-01,  1.5684e+00,\n",
      "          1.5644e+00,  2.9293e+00,  8.0692e-01,  1.4826e-01,  1.5731e+00,\n",
      "          7.9892e-01],\n",
      "        [ 5.6275e-02, -5.2479e-01,  2.2356e+00,  6.3961e-01,  2.3140e+00,\n",
      "          1.2173e+00,  7.5914e-01,  6.9601e-01, -1.1842e+00,  5.4727e-01,\n",
      "          3.7449e+00,  1.4458e+00,  1.6383e+00,  4.7106e-01,  1.7491e+00,\n",
      "          1.5757e+00],\n",
      "        [-2.0873e-01,  2.1123e+00,  4.4366e-01,  2.6953e+00,  1.0280e+00,\n",
      "          8.7227e-01, -1.1926e+00,  1.9343e-01,  1.4959e+00,  2.1047e+00,\n",
      "         -9.7144e-01, -3.2822e-01,  6.3989e-02,  1.1026e+00, -3.6426e-01,\n",
      "          2.2865e+00],\n",
      "        [-1.6026e-01,  1.9471e+00,  1.3748e+00,  2.1221e+00, -5.3691e-01,\n",
      "          1.3526e+00,  1.6077e+00, -1.2184e-01,  8.6556e-01,  1.2353e+00,\n",
      "          1.7774e+00,  7.9053e-01,  4.3712e-01,  1.6080e+00,  1.0815e+00,\n",
      "          4.3871e+00],\n",
      "        [-1.0699e+00,  9.7664e-01,  1.7699e+00,  3.9466e-01,  2.1191e+00,\n",
      "         -1.2456e+00, -3.5469e-01,  2.1998e+00,  3.5892e-01,  2.0152e+00,\n",
      "          2.4879e+00,  1.0613e+00,  9.9144e-01,  1.9586e+00,  2.4151e+00,\n",
      "          2.1847e+00],\n",
      "        [ 3.0237e+00,  1.2874e+00,  9.9830e-01,  1.5658e+00, -8.3198e-01,\n",
      "          4.1369e-01,  6.7981e-01,  1.1623e+00,  1.2602e+00,  2.6333e+00,\n",
      "          5.6840e-01,  7.8776e-01,  1.3779e+00,  2.1197e-01, -5.5069e-01,\n",
      "          1.9015e+00],\n",
      "        [-1.1151e+00,  4.7070e-01,  1.0772e-01,  1.4588e+00, -5.9742e-02,\n",
      "         -1.4456e+00,  4.3844e-01,  7.7518e-01,  1.6793e+00,  2.7561e-01,\n",
      "          2.6306e+00,  1.6805e+00,  2.6520e-01,  3.4850e-01,  3.9265e-01,\n",
      "          1.1156e+00],\n",
      "        [-2.7492e-01,  1.0018e+00,  4.1549e-01, -1.2426e-01,  8.6553e-01,\n",
      "         -3.1988e-01,  2.3810e+00, -7.1005e-03,  1.2431e+00,  2.9243e+00,\n",
      "         -2.0302e-01, -1.9436e-01,  1.2852e+00,  1.6936e+00,  1.1614e+00,\n",
      "         -5.6812e-01],\n",
      "        [ 1.6244e+00,  2.0498e+00, -4.3332e-01,  1.6092e+00,  1.7220e+00,\n",
      "          1.9624e+00,  2.6038e+00,  1.0616e+00, -4.4136e-01, -6.6838e-01,\n",
      "          9.9237e-01, -2.7782e-01,  1.5953e+00, -8.9947e-01,  1.9525e+00,\n",
      "          3.7979e-01],\n",
      "        [ 1.9891e+00,  7.7821e-01,  2.6642e+00,  7.4264e-01,  2.4709e+00,\n",
      "          1.0022e+00,  1.5744e+00,  1.4726e+00,  1.3873e+00, -1.7172e+00,\n",
      "          1.5260e+00,  1.4766e+00,  1.1219e+00,  1.7772e-01,  1.4353e+00,\n",
      "          7.6188e-01],\n",
      "        [ 4.7866e-01,  1.3956e+00,  1.1229e+00,  5.8937e-01,  1.4583e+00,\n",
      "          1.6956e+00,  2.0646e+00,  1.1531e+00,  2.4108e+00,  5.8287e-02,\n",
      "          1.1720e+00,  9.6585e-01,  6.6161e-01,  1.1503e+00, -3.9044e-02,\n",
      "          7.3885e-01],\n",
      "        [ 4.5805e-01,  1.1047e+00,  3.8387e-01,  1.3262e+00,  1.4952e+00,\n",
      "          4.3573e-02,  1.0603e+00,  6.9664e-01,  2.1443e+00,  2.0730e+00,\n",
      "          1.2653e+00,  1.0033e+00,  1.4091e+00,  5.2126e-02, -1.0669e+00,\n",
      "          1.1537e+00],\n",
      "        [ 8.1247e-01,  8.4613e-01,  6.8903e-02,  1.1186e+00,  1.3985e+00,\n",
      "          5.1067e-01,  7.1842e-01, -1.9128e-01,  2.0919e+00,  2.2694e+00,\n",
      "         -1.3171e+00,  2.2870e+00, -1.7301e+00,  1.0613e+00,  8.6220e-01,\n",
      "         -1.1654e-01],\n",
      "        [ 2.3827e+00,  4.2279e-01,  1.2515e+00,  5.8196e-01,  1.3374e+00,\n",
      "          2.7165e+00,  1.1555e+00, -1.9557e-01,  2.2438e+00,  1.2872e+00,\n",
      "          1.8778e+00,  5.0460e-01,  1.5299e-01,  5.6486e-01,  1.2311e+00,\n",
      "          1.6653e+00],\n",
      "        [ 1.7098e+00, -9.0155e-01, -5.7241e-01,  4.4431e+00, -4.4262e-02,\n",
      "          4.4438e-01,  8.1643e-01,  7.3787e-01, -4.3892e-02,  8.9846e-01,\n",
      "         -1.6517e-01,  6.1319e-01, -3.4637e-01,  6.9219e-01,  1.7190e+00,\n",
      "          3.8661e-01],\n",
      "        [ 8.4840e-01,  9.9726e-01,  9.0134e-01,  5.2246e-01,  1.7233e+00,\n",
      "          1.2798e+00,  1.2990e+00,  1.3364e-01,  1.8111e-01, -6.0158e-01,\n",
      "          1.9556e+00,  7.5878e-01,  8.1024e-01, -4.5556e-01,  1.1861e+00,\n",
      "          1.6225e+00],\n",
      "        [ 1.2456e+00,  1.1700e+00,  1.6548e+00,  2.3496e-01,  1.2566e+00,\n",
      "          6.8694e-01, -4.0076e-01,  4.2872e-02,  8.6211e-02,  2.0031e-01,\n",
      "          1.1888e-01,  5.9526e-02,  2.1513e+00,  7.0149e-01,  1.2651e+00,\n",
      "          1.0950e+00],\n",
      "        [ 1.9205e+00,  2.2162e+00,  1.5719e-01,  3.8786e-01,  9.7604e-01,\n",
      "          1.8811e+00,  3.4397e+00,  2.1908e+00,  1.5646e+00,  1.0307e+00,\n",
      "          2.5471e-01,  4.6403e-01,  7.1971e-01,  2.5195e+00,  2.3444e+00,\n",
      "          1.1107e+00],\n",
      "        [-1.1175e+00,  7.0922e-01,  4.3694e-01,  6.8427e-01,  3.4119e-01,\n",
      "          1.0669e+00,  1.1737e+00,  7.3228e-01,  6.8015e-01,  6.5728e-01,\n",
      "         -4.5499e-01,  1.4937e+00,  1.9595e+00, -4.4782e-01,  6.7998e-01,\n",
      "          5.4790e-01],\n",
      "        [ 1.3473e+00,  1.5263e+00,  1.1799e+00,  2.3212e+00, -2.7508e-01,\n",
      "          1.1980e-01,  1.3548e+00, -2.9704e-01,  2.0437e+00,  2.2453e+00,\n",
      "          1.6157e+00,  2.8571e+00,  1.6868e+00,  1.7279e+00,  3.6959e-01,\n",
      "          3.5160e-01],\n",
      "        [ 1.5855e+00,  1.0949e+00,  1.3092e+00,  2.4054e-01,  2.5509e+00,\n",
      "          8.5824e-01,  1.5323e+00,  9.8665e-01,  2.5059e+00, -1.2033e+00,\n",
      "          1.5845e-01,  2.1043e+00,  6.3594e-01,  2.1132e+00,  1.5405e+00,\n",
      "          5.3896e-01],\n",
      "        [ 2.7547e-01,  1.3239e+00, -1.0979e-01,  1.6719e+00,  7.0608e-01,\n",
      "          5.9631e-01,  2.1923e-01, -2.3231e-01,  1.9924e+00,  2.7215e+00,\n",
      "          1.1324e+00,  1.3487e+00,  1.4192e+00, -1.6390e-01,  1.4382e+00,\n",
      "          1.9069e-01],\n",
      "        [ 4.9578e-01,  3.8725e-01,  2.1093e+00, -7.2754e-01,  1.6537e+00,\n",
      "          1.9530e+00,  1.7465e+00,  2.0272e+00,  8.8338e-01, -3.8427e-01,\n",
      "          8.3876e-01,  2.2748e+00,  1.1021e+00,  5.8751e-01, -1.6993e-01,\n",
      "          3.5210e+00],\n",
      "        [ 2.9401e+00,  7.6145e-01, -3.8073e-01,  1.6863e+00,  1.2033e+00,\n",
      "         -6.8949e-01,  2.8165e+00,  2.9519e-01,  5.9429e-01, -1.4000e+00,\n",
      "          7.9592e-02,  1.0224e+00,  2.0532e+00,  1.1211e+00,  1.0129e+00,\n",
      "          2.3321e+00],\n",
      "        [ 1.6772e+00, -7.7770e-01,  2.8641e+00, -2.7162e-01,  1.5590e+00,\n",
      "          2.6602e-01, -1.6698e+00, -3.4743e-03,  7.6998e-01,  8.4128e-05,\n",
      "         -9.3400e-01,  7.8239e-01,  1.0486e+00, -2.2019e-01,  7.2434e-01,\n",
      "          1.8809e+00],\n",
      "        [ 3.6528e-01, -1.7023e-01,  1.6493e-01,  1.4313e+00, -1.1649e+00,\n",
      "          2.8494e-01,  2.1740e+00,  1.9597e+00,  9.7213e-01,  1.9648e+00,\n",
      "          1.9502e+00,  9.6931e-01,  1.2609e+00,  1.6017e+00, -1.9143e-01,\n",
      "          7.6720e-01],\n",
      "        [ 1.0691e+00,  2.5843e-01,  2.0094e+00,  5.2759e-01,  6.6640e-01,\n",
      "          1.8793e+00,  6.0411e-01,  1.8231e+00,  5.1304e-01,  1.3207e+00,\n",
      "          1.1238e+00,  1.9507e+00,  1.2973e+00,  9.2782e-01,  1.5985e+00,\n",
      "          1.7898e+00],\n",
      "        [ 1.7966e+00,  5.6236e-01,  1.0260e+00,  6.8302e-01,  1.1460e+00,\n",
      "          1.7058e+00, -2.4914e-01,  1.4937e+00,  5.1471e-01,  2.6638e+00,\n",
      "          1.2286e+00,  1.2707e+00,  3.2323e-01,  8.7248e-01,  3.1102e+00,\n",
      "          1.1453e+00],\n",
      "        [ 2.9056e+00,  2.0610e+00,  7.0973e-01,  2.7752e+00,  2.5222e+00,\n",
      "          8.1537e-01, -2.1415e-03,  4.6505e-01,  1.0650e+00,  2.4087e+00,\n",
      "          5.2010e-01,  1.5655e+00,  1.8314e+00,  1.7902e+00,  1.5157e+00,\n",
      "          2.1292e+00],\n",
      "        [ 1.0908e+00,  8.1037e-01,  2.3551e+00,  1.4264e+00,  2.1739e+00,\n",
      "          8.5098e-03,  1.7205e+00, -7.4694e-01, -4.0132e-01,  1.7777e+00,\n",
      "         -3.4620e-01,  1.8899e+00,  9.8501e-01, -2.2440e-01,  8.5761e-01,\n",
      "         -3.3267e-01],\n",
      "        [ 1.5926e+00,  1.7807e+00,  1.4635e+00,  2.4228e+00,  3.5531e+00,\n",
      "          9.7628e-01,  1.3119e+00,  2.5935e+00,  1.7062e+00,  3.2392e-01,\n",
      "          1.6018e+00,  1.3003e+00, -4.1513e-02,  1.9560e+00,  1.4130e-01,\n",
      "          1.4567e+00],\n",
      "        [ 8.4148e-01,  1.5388e+00,  3.2001e+00,  9.7184e-01,  2.3852e+00,\n",
      "         -3.4627e-01, -3.2973e-01,  1.2879e+00,  1.0706e+00,  7.9462e-01,\n",
      "          1.8033e+00,  1.3260e+00,  1.3082e+00,  8.4562e-01,  1.5487e+00,\n",
      "          2.8934e+00],\n",
      "        [ 9.4758e-01,  2.6312e+00,  4.9286e-01,  1.1799e+00,  1.3127e-01,\n",
      "          1.2117e+00, -9.9677e-01,  1.2221e+00,  1.0855e+00,  2.0590e+00,\n",
      "          2.3891e+00,  4.4053e-01,  7.9866e-02,  2.0192e+00,  1.0548e+00,\n",
      "          1.2394e+00],\n",
      "        [ 2.3431e+00,  1.6465e+00, -1.7703e-01,  3.9546e-01,  1.5788e+00,\n",
      "          2.5106e-02,  1.8616e+00,  8.9900e-01,  1.1200e+00,  2.2182e+00,\n",
      "          2.0319e+00,  3.6811e-02,  1.5060e+00,  8.8353e-01,  1.5698e+00,\n",
      "          7.5744e-01],\n",
      "        [ 4.0559e-01,  1.4804e+00,  1.2104e+00, -1.5037e-01,  1.1699e-01,\n",
      "          1.0905e-01,  1.7845e+00, -3.0890e-02,  1.0156e+00,  1.5314e-01,\n",
      "          2.1729e+00,  3.5927e-01,  4.7891e-01,  1.1659e+00,  6.1474e-01,\n",
      "          1.6967e+00],\n",
      "        [ 1.9322e+00,  2.7654e-02,  9.7079e-01,  5.0793e-01, -7.1325e-01,\n",
      "          1.5723e+00,  8.1467e-01,  1.2100e+00,  1.7069e+00,  7.9232e-01,\n",
      "          3.3282e-01,  9.1052e-01,  1.8910e-01,  2.0107e+00,  2.4690e+00,\n",
      "          2.7118e+00],\n",
      "        [ 1.4299e+00,  1.1589e+00,  1.5701e+00,  2.3131e+00,  1.3025e+00,\n",
      "          2.5221e+00,  3.3215e-02,  7.7645e-01,  1.6953e+00,  2.5878e+00,\n",
      "         -7.1594e-01,  4.0098e-01,  1.4377e+00,  1.1724e+00,  1.3607e-01,\n",
      "          4.7565e-01],\n",
      "        [ 1.3791e+00,  2.3849e+00,  2.9308e+00,  9.0676e-02, -5.0648e-01,\n",
      "          1.3254e+00,  1.1379e-01, -9.5677e-03,  9.2728e-01, -5.6395e-01,\n",
      "          1.8066e+00,  1.8716e+00,  9.2665e-01,  7.5372e-01,  7.0744e-01,\n",
      "          9.2864e-01],\n",
      "        [ 2.6856e-02,  2.0233e+00,  5.6148e-01,  2.2386e+00,  8.7219e-01,\n",
      "         -5.8496e-02,  1.0172e+00,  2.1679e+00,  5.2174e-01,  4.9799e-01,\n",
      "          1.0259e+00,  8.2525e-01,  1.1028e+00,  1.2951e+00,  1.2620e-01,\n",
      "          4.9575e-01],\n",
      "        [ 3.8288e-01,  1.8398e+00,  6.2276e-01, -3.2654e-01,  3.7301e-01,\n",
      "          2.0307e+00,  8.5236e-02,  2.9538e+00,  6.0731e-01,  9.7176e-01,\n",
      "          6.8193e-01,  5.4905e-01,  2.5887e+00,  1.9637e+00,  8.0697e-01,\n",
      "          3.6000e+00],\n",
      "        [ 2.3865e+00,  1.7497e-02, -7.8366e-01, -1.1766e-01,  1.7119e+00,\n",
      "          2.9764e-02,  2.5191e+00, -2.8191e-01,  5.3326e-01,  2.8548e+00,\n",
      "          2.1404e+00, -7.1627e-01,  4.7504e-01, -2.1862e-01,  1.2727e+00,\n",
      "          5.9573e-01],\n",
      "        [ 1.4719e+00,  9.1578e-01,  1.5756e+00,  1.3805e+00, -6.8549e-01,\n",
      "          6.5655e-03,  1.6954e+00,  1.0423e+00, -7.1984e-01,  1.2639e-01,\n",
      "          1.7385e+00,  9.8733e-01,  1.4782e+00, -1.4249e+00,  2.4037e+00,\n",
      "          4.6492e-01],\n",
      "        [ 2.0317e+00,  1.7914e+00,  1.9497e-03,  1.7960e+00,  3.1084e+00,\n",
      "          1.9312e-01,  9.8300e-01,  1.3365e+00, -1.8543e-01,  9.9442e-01,\n",
      "          7.8605e-02,  6.5190e-01,  2.0060e+00,  2.3516e+00, -1.8479e+00,\n",
      "         -3.1798e-01],\n",
      "        [ 1.4759e+00,  1.8986e+00,  2.1825e+00,  1.7925e+00,  1.9394e-01,\n",
      "         -2.0857e-01,  7.3448e-01,  5.3144e-01,  2.3110e+00,  9.4551e-01,\n",
      "          4.2706e-01,  1.6146e+00,  1.4372e+00,  5.4160e-02,  1.0111e-01,\n",
      "          6.8330e-01],\n",
      "        [ 2.7583e-01,  1.2059e+00, -1.0066e+00,  2.1710e+00,  9.3683e-01,\n",
      "          2.0729e+00,  1.5504e+00,  2.3549e+00, -8.5334e-01,  3.6904e-01,\n",
      "          2.8384e+00,  2.5930e+00, -3.5579e-01,  5.8832e-01,  2.6013e+00,\n",
      "          1.9927e+00],\n",
      "        [ 3.8052e-01,  7.1411e-01,  1.4133e+00,  1.7080e+00,  3.4059e+00,\n",
      "          1.0830e+00,  2.1969e-01,  2.5824e+00,  2.4773e-01, -2.7922e-01,\n",
      "          1.8413e+00, -1.4428e-01,  1.0537e+00,  3.5783e-01,  7.0445e-01,\n",
      "         -8.0459e-01],\n",
      "        [-7.9086e-02,  7.2913e-01,  2.2500e+00,  1.0007e+00,  1.4874e+00,\n",
      "          1.4273e+00,  3.5483e-01,  2.8059e+00,  4.1025e-01,  1.7634e+00,\n",
      "         -2.4972e-01,  1.2184e+00, -3.7443e-01,  1.7486e+00,  8.3892e-01,\n",
      "          2.6971e+00],\n",
      "        [ 5.3707e-01,  1.6947e-01,  2.3064e-01,  7.0139e-01,  1.8140e+00,\n",
      "          3.1898e+00,  2.2927e+00,  1.9750e+00,  5.9591e-01,  3.8697e-01,\n",
      "         -2.3330e-01,  1.6850e-01,  2.1931e+00, -5.9409e-01,  1.3671e+00,\n",
      "          4.6816e-01],\n",
      "        [ 2.1356e+00,  3.5038e-01,  2.5162e+00,  5.7312e-02,  1.1883e+00,\n",
      "          3.0379e+00,  9.9602e-01,  1.0914e+00, -6.2209e-01,  9.9754e-01,\n",
      "         -1.0686e-01, -4.2975e-02, -2.9010e-01,  1.5196e+00, -6.5717e-01,\n",
      "         -9.4112e-02],\n",
      "        [ 3.1622e+00,  6.6885e-01, -2.5747e-01,  6.1704e-02,  1.8978e+00,\n",
      "          9.8085e-01,  9.7250e-01,  2.4557e+00, -7.2807e-01,  2.8280e+00,\n",
      "          2.7297e+00,  5.8278e-01,  1.6470e+00,  2.2107e-01, -5.9223e-01,\n",
      "          8.9067e-01],\n",
      "        [ 7.2728e-01,  8.6823e-01,  1.5535e+00,  1.8976e+00,  2.5839e-01,\n",
      "          1.0247e+00,  1.8111e+00,  7.0102e-01,  2.3456e+00,  2.6166e+00,\n",
      "          1.9482e-01,  9.2305e-01,  2.5876e+00,  1.6153e+00,  9.1799e-01,\n",
      "          2.5890e+00],\n",
      "        [ 4.0500e+00,  5.8702e-01,  1.6658e+00,  1.9341e+00,  1.7417e+00,\n",
      "         -1.4246e-01, -2.2414e-02,  1.0503e+00,  4.1411e-01,  6.0699e-01,\n",
      "          1.4311e+00,  2.0309e+00, -6.1773e-01,  2.6603e+00,  3.0016e+00,\n",
      "          1.1176e+00],\n",
      "        [ 5.3636e-01,  3.9210e+00,  8.1380e-02,  2.3165e+00,  3.0010e+00,\n",
      "          3.7216e-01,  1.4655e+00,  7.6447e-01,  2.2697e+00,  9.6846e-01,\n",
      "          8.5708e-01,  1.7011e+00,  5.1991e-01, -2.4524e-01, -2.6905e-01,\n",
      "          1.6746e+00],\n",
      "        [-1.1213e+00,  1.1368e+00,  1.9406e+00, -1.9870e-01,  1.4911e+00,\n",
      "          2.4120e+00,  1.4707e+00,  7.9021e-01, -1.2272e+00,  2.3053e+00,\n",
      "         -6.2893e-02,  5.1643e-01,  6.8710e-01,  5.8940e-01, -5.5945e-01,\n",
      "          1.7331e+00],\n",
      "        [ 7.5106e-01,  4.7168e-01, -5.9939e-01, -1.6811e-02,  1.4925e+00,\n",
      "          3.8901e-01,  2.9002e-01,  2.3394e+00,  1.8861e+00,  1.1707e-01,\n",
      "          2.1867e+00,  1.3827e-01,  9.3847e-01,  1.2861e+00,  3.2367e+00,\n",
      "          4.4920e-01]])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 3.],\n",
      "         [0., 0., 0.,  ..., 0., 3., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 3.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 3., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 3., 0.],\n",
      "         [0., 0., 0.,  ..., 3., 0., 3.],\n",
      "         [0., 0., 0.,  ..., 0., 3., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "inpdat,outdat = matthew_to_torch(data_L16)\n",
    "print(inpdat.shape)\n",
    "print(outdat.shape)\n",
    "print(inpdat.dtype)\n",
    "print(outdat.dtype)\n",
    "inpdat = inpdat.to(torch.float32)\n",
    "outdat = outdat.to(torch.float32)\n",
    "print(inpdat.dtype)\n",
    "print(outdat.dtype)\n",
    "print(inpdat)\n",
    "print(outdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "087937fb-9ceb-49b2-9f1f-07acae1fb59b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "31\n",
      "Predicted output for new input:\n",
      " tensor([[[-7.7747e-03,  2.1380e-01, -1.6585e-01, -1.0024e-01,  1.6139e-01,\n",
      "          -6.0152e-02, -1.1579e-01, -7.3285e-02, -2.7445e-01,  2.8841e-01,\n",
      "          -3.3634e-01, -1.5521e-01, -1.2954e-01,  1.1629e-01, -4.5117e-01,\n",
      "           1.9926e-01,  3.8311e-02,  2.7376e-01, -3.3543e-01,  4.8679e-01,\n",
      "           1.8267e-01,  4.2103e-02, -2.7377e-01,  5.4501e-01,  1.2434e-01,\n",
      "           2.9306e-01,  3.9964e-01, -4.1127e-02, -1.9464e-02,  3.5240e-01,\n",
      "          -2.2735e-01],\n",
      "         [-5.3218e-01,  5.2189e-02, -3.8545e-01,  4.3338e-01,  1.0758e-01,\n",
      "          -2.4636e-01, -2.5366e-01,  3.8107e-03,  1.5976e-01,  4.0606e-01,\n",
      "          -2.5935e-01,  3.3557e-02,  2.9346e-01,  5.3315e-02,  2.2922e-01,\n",
      "           2.6410e-01, -1.5339e-01,  3.4642e-01,  5.7444e-05,  3.1129e-01,\n",
      "           4.6756e-01, -2.5541e-01, -5.8440e-02, -3.5606e-01, -2.6080e-01,\n",
      "          -1.6698e-01,  1.7248e-01,  2.0310e-01,  3.3687e-01, -2.7141e-01,\n",
      "           2.2077e-01],\n",
      "         [ 5.2132e-02,  2.6543e-02, -3.3871e-01,  1.9862e-01,  1.9725e-01,\n",
      "           3.5999e-01, -8.1916e-02, -2.8398e-01,  1.6887e-01, -7.9638e-03,\n",
      "          -3.3491e-01, -9.3661e-03, -2.4617e-01, -1.8560e-01, -7.9673e-02,\n",
      "          -2.2618e-01, -2.7136e-01, -7.6632e-03, -1.4801e-01, -1.2976e-01,\n",
      "           1.3848e-01, -2.2948e-01,  2.7880e-01, -3.6180e-01,  5.7469e-02,\n",
      "           4.8048e-01,  7.7983e-02, -1.5704e-01, -2.7547e-01, -1.3815e-01,\n",
      "          -1.2768e-01],\n",
      "         [ 6.7542e-02, -1.4679e-02, -4.3776e-01,  1.1668e-01,  4.7139e-02,\n",
      "          -1.9345e-01,  1.8317e-01,  2.6687e-01,  7.5507e-02,  2.8205e-01,\n",
      "           7.1733e-02, -3.1803e-01, -2.8356e-01, -1.7938e-01,  8.0521e-03,\n",
      "          -3.6623e-01,  6.7146e-02, -2.9789e-01,  1.4126e-01,  2.0322e-02,\n",
      "           7.7129e-02, -3.5895e-01, -4.5091e-02, -2.0376e-01,  1.0291e-01,\n",
      "          -3.9261e-01,  2.4785e-01,  2.0509e-01, -1.1441e-01,  1.5130e-01,\n",
      "           4.8731e-02],\n",
      "         [ 1.8900e-01,  1.5105e-01,  1.8289e-01,  6.8928e-02, -2.6711e-01,\n",
      "           1.8218e-01,  7.7688e-02, -1.0319e-01,  3.4537e-02, -8.6469e-02,\n",
      "          -1.3232e-01,  3.0486e-01,  2.3295e-01, -5.9815e-02,  1.5761e-01,\n",
      "          -1.9717e-01, -1.7968e-03, -2.7746e-01,  1.6208e-01,  5.7297e-02,\n",
      "           1.5688e-01,  2.4799e-01, -1.7221e-01, -2.4242e-02, -1.4315e-02,\n",
      "          -2.9830e-01, -4.5149e-02,  3.1886e-01,  2.4110e-01,  4.7567e-02,\n",
      "          -7.7345e-02],\n",
      "         [ 2.7972e-01,  4.0486e-01, -2.7381e-01, -2.0747e-01,  4.3840e-01,\n",
      "          -1.0989e-01, -2.3423e-01,  2.4750e-01,  4.2131e-02, -2.6175e-01,\n",
      "           5.4501e-01, -3.0763e-02, -1.5013e-01,  3.2767e-01, -1.6719e-01,\n",
      "          -3.8941e-02,  4.8239e-01, -1.9595e-02,  2.1345e-01, -1.9691e-01,\n",
      "          -2.4027e-02, -8.7254e-03, -8.7705e-02, -2.7801e-01,  1.1106e-01,\n",
      "           7.9573e-02,  3.5351e-01, -1.1584e-01, -2.7404e-02, -4.7947e-01,\n",
      "          -4.8393e-03],\n",
      "         [ 6.3710e-02,  3.1699e-01,  4.8941e-01, -1.8690e-01,  2.3995e-02,\n",
      "           6.8953e-02, -2.4868e-01, -3.4996e-01, -1.5030e-03, -1.7895e-01,\n",
      "           1.4098e-01, -5.7025e-02,  2.6423e-01, -2.1730e-01, -1.1180e-01,\n",
      "           3.9035e-02, -2.7295e-02,  1.0484e-01,  3.9665e-01,  3.0955e-01,\n",
      "           7.8899e-02, -1.4512e-01,  2.8997e-01, -2.9787e-01,  1.9081e-02,\n",
      "          -9.9732e-02,  2.2274e-01, -3.9437e-01, -4.1144e-01,  8.4593e-02,\n",
      "          -1.9626e-02],\n",
      "         [-2.4679e-01, -2.4420e-01,  1.0568e-01, -5.2218e-02, -3.2186e-01,\n",
      "           4.7528e-01,  5.8583e-01,  2.1723e-02, -3.6446e-01,  1.1832e-01,\n",
      "           2.0805e-01, -2.7304e-02,  2.3181e-01,  1.2385e-01,  9.3083e-02,\n",
      "           3.1232e-01, -1.1517e-02, -6.1004e-02,  1.2931e-01, -1.2478e-01,\n",
      "           1.5868e-01,  1.9269e-01,  5.1943e-01, -3.2935e-02, -2.5327e-02,\n",
      "           3.2282e-01, -4.0690e-01,  2.2934e-01,  2.1998e-01,  2.1759e-03,\n",
      "          -1.7924e-01],\n",
      "         [-1.5301e-01, -2.2337e-02, -3.1027e-01,  8.9516e-02, -1.2780e-01,\n",
      "          -2.0144e-01, -4.1224e-02, -7.2641e-02,  3.5302e-01,  2.1640e-02,\n",
      "           3.8088e-01, -2.4244e-01, -4.5566e-02,  8.2548e-02,  2.9654e-02,\n",
      "           3.7292e-01,  2.3743e-01, -2.4226e-01, -3.2825e-01, -3.4845e-01,\n",
      "          -3.8530e-01,  7.4782e-02, -2.2346e-01, -3.3734e-01,  9.2495e-03,\n",
      "           1.2576e-01,  1.8615e-01,  4.8317e-02, -2.4525e-02, -2.0088e-01,\n",
      "          -2.9832e-03],\n",
      "         [-3.4371e-02,  1.5540e-01,  2.3863e-01, -6.5547e-03, -3.1879e-01,\n",
      "           1.4527e-01,  4.2780e-02, -1.2448e-01,  1.9965e-01,  1.9138e-01,\n",
      "          -2.0093e-01,  4.7146e-01,  1.5655e-01, -9.7204e-02,  3.3584e-01,\n",
      "           3.2354e-01, -3.4530e-01,  6.7251e-02,  4.0050e-01,  5.1658e-03,\n",
      "          -2.0487e-01, -3.1668e-02,  5.8484e-02,  3.0808e-01, -8.1954e-02,\n",
      "           2.5629e-01,  2.7290e-01,  2.5236e-01,  9.4221e-02, -1.3024e-01,\n",
      "          -1.4932e-01],\n",
      "         [ 2.8174e-01,  2.8529e-01,  9.1419e-02, -3.8121e-01, -1.5522e-02,\n",
      "          -6.8311e-02,  2.3719e-01, -1.3226e-01, -2.5987e-02,  4.0714e-01,\n",
      "           2.3577e-01, -4.8194e-02,  2.3935e-01, -1.9151e-02, -1.3203e-01,\n",
      "          -8.4199e-03, -2.6094e-01, -2.0649e-01,  4.7055e-02,  1.6190e-01,\n",
      "          -2.4602e-01, -1.0548e-01,  1.1834e-01,  1.9799e-01, -2.5370e-01,\n",
      "          -2.0795e-01,  9.8366e-02,  7.8204e-03, -3.9858e-01, -2.8886e-01,\n",
      "          -1.0641e-01],\n",
      "         [ 7.2868e-02, -2.6138e-01,  3.3948e-01, -3.3933e-01, -1.5935e-01,\n",
      "          -3.9548e-02, -1.7602e-01,  3.2209e-01,  9.8909e-02,  4.9624e-02,\n",
      "           1.5086e-01, -2.3938e-01,  3.1655e-01,  2.4666e-01, -1.1218e-01,\n",
      "           2.4772e-01, -2.1560e-01,  2.8336e-01, -2.6838e-02,  7.2736e-02,\n",
      "          -1.0118e-01,  2.2058e-01,  2.3368e-01, -1.8485e-01, -2.8973e-01,\n",
      "           2.2183e-01,  2.0063e-01, -2.1968e-01,  4.8367e-02,  1.6407e-01,\n",
      "           6.3543e-02],\n",
      "         [-2.8061e-01, -9.0741e-02,  1.1887e-01,  3.3838e-02, -2.4019e-02,\n",
      "          -8.7449e-02,  3.4103e-01, -1.2918e-01, -3.3202e-02, -3.5351e-01,\n",
      "          -1.5686e-01,  8.1554e-02, -2.9518e-01, -4.1000e-01, -3.2716e-01,\n",
      "           2.0357e-01,  1.6291e-01, -1.7531e-01,  5.5267e-02, -3.5548e-01,\n",
      "           2.3278e-01,  1.2857e-01,  3.1989e-02,  6.9724e-02,  2.4589e-01,\n",
      "          -2.6216e-01,  6.4290e-02, -4.6224e-01,  1.2403e-01, -1.2018e-01,\n",
      "          -2.8776e-01],\n",
      "         [-2.6833e-01, -3.2254e-01, -3.8617e-02,  1.1678e-01, -4.6649e-01,\n",
      "           1.5510e-01,  8.0676e-02,  1.0135e-01,  2.3483e-01, -1.0520e-01,\n",
      "           7.5526e-02,  3.6801e-02,  2.0376e-01,  3.5217e-02, -3.6462e-02,\n",
      "          -3.5398e-01,  2.4044e-01, -3.2886e-01, -3.2105e-01, -4.7717e-01,\n",
      "           1.7316e-01, -4.3444e-02, -4.3774e-02, -1.8456e-01,  2.7769e-01,\n",
      "           2.0879e-01,  1.2420e-01, -8.7023e-02,  3.3327e-01, -9.8428e-02,\n",
      "           1.6332e-01],\n",
      "         [-2.7056e-01,  2.0179e-01, -1.7309e-01, -8.3720e-02,  3.5111e-01,\n",
      "           4.1206e-02, -1.5389e-01, -1.0443e-02,  8.3557e-02,  8.2264e-02,\n",
      "           3.7175e-02, -4.0285e-02,  2.6476e-01,  2.5219e-01,  4.6725e-01,\n",
      "          -3.0812e-01,  1.0110e-01,  5.7244e-01,  7.8445e-02,  9.2278e-02,\n",
      "           6.4546e-02,  9.2358e-03,  5.0515e-02,  2.3414e-01,  1.5214e-01,\n",
      "           6.3834e-02, -3.2559e-01, -1.2059e-01,  1.9604e-01, -2.9486e-01,\n",
      "           1.8143e-01],\n",
      "         [-8.0782e-02,  3.3219e-03, -3.1401e-01, -9.6181e-02,  4.2930e-01,\n",
      "           1.2501e-01, -1.7395e-01,  3.0962e-01,  1.2894e-01, -2.5151e-02,\n",
      "           1.7784e-02, -3.8650e-01,  8.8543e-02,  4.1958e-01,  4.7584e-01,\n",
      "          -3.1587e-02, -2.5457e-01,  2.3478e-02,  6.6282e-02,  9.8392e-02,\n",
      "          -5.7826e-02,  1.6807e-01,  4.6701e-02, -1.3154e-01,  8.5575e-02,\n",
      "          -8.0413e-02, -2.2673e-01, -4.0854e-01,  2.8327e-01, -3.2466e-01,\n",
      "           5.7102e-01],\n",
      "         [-3.4892e-03,  1.7376e-01,  4.0758e-01,  4.5197e-02,  1.1545e-01,\n",
      "           2.5845e-01, -2.8788e-01,  1.8771e-01, -3.0061e-01, -2.0070e-01,\n",
      "           2.9111e-01, -1.9128e-01, -7.2873e-02,  1.9062e-01,  1.6346e-01,\n",
      "          -1.0915e-01, -2.8008e-01, -1.5610e-01, -3.1773e-01, -6.0490e-02,\n",
      "          -3.5238e-01, -5.0339e-02, -1.8685e-01,  2.4210e-01,  4.5057e-02,\n",
      "          -3.0169e-01,  1.9189e-02, -1.4757e-01, -2.2465e-01,  1.0217e-01,\n",
      "           4.1044e-01],\n",
      "         [ 7.9050e-02, -2.6277e-01, -1.3502e-01, -1.5609e-01, -3.4651e-02,\n",
      "          -2.6573e-01, -3.5293e-01, -6.5958e-02,  1.3944e-01, -2.6072e-01,\n",
      "           1.1464e-01, -3.5020e-01, -9.5230e-02, -2.6708e-01,  2.4040e-01,\n",
      "           1.7515e-01,  2.3869e-02,  3.4582e-01, -4.2324e-01,  4.5617e-02,\n",
      "           1.7429e-01, -3.9952e-01,  1.7324e-01, -3.3178e-02,  1.6449e-01,\n",
      "           2.7111e-01, -5.7461e-02, -6.1054e-02,  1.3960e-02, -4.6034e-01,\n",
      "           3.3510e-01],\n",
      "         [ 4.6068e-02,  8.7840e-02,  5.1196e-01, -4.6324e-01,  4.0046e-01,\n",
      "           4.9292e-01,  2.9311e-02,  1.4890e-01,  8.5782e-02, -3.8209e-01,\n",
      "          -1.0371e-01, -1.2926e-01, -1.9998e-01,  1.3644e-01, -3.2193e-01,\n",
      "          -2.8473e-01,  1.6752e-01, -2.5959e-01, -8.6825e-02,  3.7805e-02,\n",
      "           3.4589e-01,  2.4624e-01, -2.5895e-01,  2.6862e-01, -5.9857e-02,\n",
      "          -2.0915e-01, -5.6815e-02, -5.6141e-02, -2.6262e-01, -1.7951e-04,\n",
      "          -1.3809e-01],\n",
      "         [ 9.0448e-02,  1.8334e-01, -8.6529e-02,  1.3445e-01, -6.6609e-02,\n",
      "           3.5011e-01, -3.4158e-01, -6.4529e-02,  1.4612e-01, -2.0213e-01,\n",
      "          -2.2692e-01, -3.3826e-01, -3.9360e-02,  2.5591e-01,  4.0035e-01,\n",
      "           2.2326e-01, -3.5849e-02,  4.1127e-02, -2.1232e-01,  1.9459e-01,\n",
      "           3.2132e-02,  2.9176e-01, -5.7372e-02, -8.8455e-03,  3.2366e-01,\n",
      "           2.1626e-01,  1.6091e-01,  1.8745e-02, -2.5400e-01, -3.9204e-01,\n",
      "           4.5073e-01],\n",
      "         [-4.0094e-01,  3.6179e-02,  2.9362e-02, -2.6220e-01,  1.9048e-01,\n",
      "          -1.9929e-02, -7.7360e-02, -2.4535e-01,  1.0135e-01, -7.0654e-03,\n",
      "          -3.2246e-01, -4.8780e-01,  2.1500e-01,  2.5311e-01,  1.0450e-01,\n",
      "          -3.0300e-01,  3.6169e-02,  3.1646e-01,  3.2711e-02,  4.2056e-02,\n",
      "          -1.0078e-01, -3.4362e-02,  8.8961e-02, -2.0288e-01, -3.2634e-01,\n",
      "          -1.3142e-01, -5.0931e-01, -1.6966e-01,  2.6751e-02, -7.2321e-02,\n",
      "          -6.0862e-02],\n",
      "         [ 4.5052e-02, -1.2889e-01,  1.4513e-01, -5.9803e-02, -3.3086e-01,\n",
      "          -2.1168e-01, -1.1920e-01,  4.1574e-02, -1.7165e-01,  1.0286e-01,\n",
      "          -7.0906e-02, -1.6066e-01,  1.9770e-01, -6.5405e-02,  1.0945e-01,\n",
      "           3.4553e-01, -3.8497e-02,  2.2694e-01,  3.8842e-01, -9.5961e-02,\n",
      "           3.3766e-01,  1.9844e-01, -1.9178e-01, -3.7889e-01,  1.1975e-01,\n",
      "           8.5145e-02,  2.6806e-03,  1.9194e-01,  1.3200e-01,  3.1726e-01,\n",
      "           2.3226e-01],\n",
      "         [-2.1406e-02,  2.9900e-01,  4.6856e-01, -3.3051e-01, -1.5013e-01,\n",
      "          -4.2416e-01, -1.3134e-01, -1.5147e-01, -2.6115e-01, -1.9655e-01,\n",
      "           2.4228e-01,  2.0185e-01, -2.2132e-01,  4.3704e-01,  5.8738e-02,\n",
      "           7.7703e-02, -1.3409e-01, -2.1233e-02, -4.4354e-02,  3.2862e-01,\n",
      "          -3.9304e-01,  3.7150e-01, -7.4319e-02,  1.3822e-01, -2.1771e-01,\n",
      "           1.3953e-01,  6.5802e-02, -1.0395e-01,  4.0745e-03, -1.2970e-01,\n",
      "           1.4345e-01],\n",
      "         [ 4.8868e-01,  1.6820e-01, -4.4400e-02,  4.7604e-02, -1.0732e-01,\n",
      "           1.8718e-01,  1.3724e-01, -7.9168e-02, -1.6245e-01, -2.4059e-01,\n",
      "          -1.3630e-02, -6.0224e-01, -2.5379e-01,  2.8528e-02,  2.0866e-01,\n",
      "           1.0748e-01, -4.0195e-01, -1.9158e-01,  3.1216e-01,  3.9828e-01,\n",
      "          -1.5018e-01,  2.6731e-02, -8.0696e-02,  2.2884e-02,  4.8863e-02,\n",
      "           3.4573e-01, -3.5568e-01,  2.9149e-01,  1.0982e-01, -3.7544e-01,\n",
      "           3.4982e-01],\n",
      "         [ 8.8725e-03, -1.8445e-01, -1.5964e-01, -2.4283e-01,  1.5156e-01,\n",
      "           6.3601e-02, -3.4389e-01,  9.1637e-02,  1.7684e-01,  1.2967e-01,\n",
      "           1.3314e-02,  1.7339e-01, -1.5345e-01, -1.9074e-01, -5.9060e-02,\n",
      "           2.0687e-01,  4.1141e-01, -2.1369e-01, -7.9140e-02, -5.6039e-02,\n",
      "          -3.5949e-02, -2.2621e-01, -4.1734e-01,  4.2146e-01,  2.5155e-01,\n",
      "           4.2673e-01,  3.7417e-01,  2.3762e-01, -1.9955e-01, -2.5538e-01,\n",
      "           6.0102e-02],\n",
      "         [ 9.1917e-02,  5.7278e-01,  3.0395e-01,  2.1295e-02,  2.3181e-01,\n",
      "           2.0064e-01,  2.9879e-01,  2.1744e-03, -9.3188e-02,  1.1406e-01,\n",
      "          -1.4644e-02,  4.5565e-02, -7.7195e-02, -1.7743e-01,  1.9385e-01,\n",
      "           1.8495e-02,  1.0244e-01, -1.0571e-01, -7.7708e-03,  3.8406e-01,\n",
      "          -2.4015e-01, -3.2405e-02,  5.1338e-03,  6.2336e-02, -4.1138e-01,\n",
      "          -1.2849e-01,  1.0701e-01, -1.0316e-01,  2.8034e-01, -6.4176e-02,\n",
      "          -9.3607e-02],\n",
      "         [ 3.3067e-01, -2.3165e-01, -2.7275e-02,  3.1928e-01, -8.4014e-02,\n",
      "          -1.6393e-01, -1.0697e-01, -1.8465e-01,  2.1000e-01,  1.0859e-01,\n",
      "           4.2724e-02,  2.0645e-02, -2.6351e-01,  1.7319e-02,  3.2745e-01,\n",
      "          -2.8414e-01,  2.5162e-01,  3.1054e-01,  9.6330e-03, -7.9208e-02,\n",
      "           3.0362e-02,  1.8123e-01, -2.1737e-03, -1.4259e-01, -2.3271e-01,\n",
      "           1.4532e-01,  2.4144e-01,  1.1646e-02,  9.0169e-02, -1.1527e-01,\n",
      "          -2.4889e-01],\n",
      "         [ 1.6354e-01, -2.0031e-01,  1.8000e-01, -2.4755e-01, -3.2452e-01,\n",
      "          -3.0129e-01,  1.6993e-01, -4.4859e-01, -8.4724e-02,  5.5785e-02,\n",
      "           6.7233e-02,  2.8040e-01,  3.2368e-01, -3.9930e-01, -1.0746e-01,\n",
      "          -1.2455e-01, -2.7194e-01,  3.7120e-01, -1.1083e-01,  1.9991e-01,\n",
      "          -2.6902e-01,  7.7800e-02, -1.1522e-01, -1.1613e-02, -3.3292e-01,\n",
      "          -3.1291e-01, -1.4190e-01,  2.9944e-01,  2.3943e-01, -1.7225e-01,\n",
      "          -2.4265e-01],\n",
      "         [-5.8101e-02, -6.5773e-02, -8.2006e-02, -2.1594e-01, -7.4863e-02,\n",
      "          -2.4221e-01,  1.8284e-01, -5.6240e-02,  1.9350e-01,  4.8263e-01,\n",
      "          -2.0937e-01, -2.8996e-01, -4.8748e-02, -1.1581e-01, -6.0333e-02,\n",
      "           8.5140e-02,  2.3443e-01, -1.8496e-01,  5.3186e-02,  1.2755e-01,\n",
      "          -3.1860e-01, -1.7240e-01,  6.4908e-02,  1.3081e-01,  7.9392e-02,\n",
      "           2.5847e-02,  3.8023e-02, -4.0480e-01,  1.9178e-01, -2.2811e-02,\n",
      "           7.6222e-02],\n",
      "         [ 7.7469e-02,  3.7868e-01, -1.2266e-01,  3.9685e-02, -1.9911e-02,\n",
      "          -5.9554e-02,  1.8938e-01,  3.4575e-02,  4.1435e-01, -1.5253e-01,\n",
      "          -1.5426e-01, -4.3943e-02,  3.2190e-01,  3.7045e-02, -1.2705e-01,\n",
      "          -1.3811e-02, -4.4667e-01,  1.7715e-01,  2.2911e-01, -2.7749e-01,\n",
      "           1.5451e-01, -3.1253e-01,  2.1079e-01, -3.2657e-03, -1.2721e-01,\n",
      "           1.9637e-01, -1.7650e-01, -1.4828e-01,  3.4517e-02, -1.2886e-02,\n",
      "           1.1155e-01],\n",
      "         [ 1.0216e-01,  1.2009e-01, -2.4846e-01,  3.2714e-01, -2.8668e-01,\n",
      "          -1.7835e-01,  6.5521e-02, -4.2438e-02,  1.9846e-02,  1.3315e-01,\n",
      "          -2.2433e-01,  5.3609e-02,  5.5885e-02,  1.0423e-01,  2.3585e-01,\n",
      "           3.3486e-01,  7.6150e-02, -5.4564e-02,  9.9790e-02, -2.3156e-01,\n",
      "           2.4439e-01, -5.6028e-02,  3.6372e-02,  2.9418e-01,  2.7074e-01,\n",
      "           4.6853e-02, -1.0877e-01, -1.2252e-01, -7.9326e-03, -1.6827e-01,\n",
      "           3.1465e-01]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dim  = inpdat.shape[1]\n",
    "output_dim = outdat.shape[1]\n",
    "print(input_dim)\n",
    "print(output_dim)\n",
    "\n",
    "# Define the model class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # Second hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim * output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through hidden layers with activation\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        # Output layer without activation (linear regression)\n",
    "        return self.fc3(x).view(x.shape[0], output_dim, output_dim)\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(inpdat, outdat)\n",
    "dataloader = DataLoader(dataset, batch_size=4)  # Adjust batch_size as needed\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = MLP(inpdat.shape[1], 16, outdat.shape[1])  # Adjust hidden layer size\n",
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):  # Adjust number of epochs for better training\n",
    "    for data, target in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Example usage: predict output for a new input\n",
    "new_input = torch.tensor([list(range(16))]).to(torch.float32)\n",
    "predicted_output = model(new_input)\n",
    "print(f\"Predicted output for new input:\\n {predicted_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7dd67f-a5eb-49fd-a076-629726642d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cee57-192f-48d0-8f0c-96f1f47d17fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0490e-626c-4be5-a34c-1f663a24644d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e56930e-510c-44e8-8452-4c9b596d27db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a9962-ed25-4761-9317-e27506b48cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0131129-d0c7-47bd-b75a-4420a27b37b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4627f6-3c16-4a8e-92de-baf4f8ebf60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80bd67-a119-4bbb-ba9f-c9c93d265305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad44dae-a378-4966-a8b2-4a733035e17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e5ed3-eaf5-41d5-ae65-7907e34eed3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6b23f-9044-45bf-a007-d39f0319745f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
